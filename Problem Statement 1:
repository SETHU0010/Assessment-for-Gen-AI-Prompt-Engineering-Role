# Install necessary libraries
# !pip install spacy
# !python -m spacy download en_core_web_sm

import spacy

# Load spaCy's English tokenizer, POS tagger, parser, NER, and word vectors
nlp = spacy.load('en_core_web_sm')

# Function to preprocess and tokenize text using spaCy
def preprocess_text_spacy(text):
    """
    Preprocess and tokenize text using spaCy.
    
    Args:
        text (str): The input text to preprocess.
        
    Returns:
        List of tokens (List[str]): A list of cleaned and tokenized words.
    """
    # Process the text with spaCy
    doc = nlp(text)
    
    # Tokenize, remove punctuation, stop words, and normalize case
    tokens = [token.lemma_.lower() for token in doc if not token.is_punct and not token.is_stop]
    
    return tokens

# Test the function
text = "Hello! This is a sample text with punctuation, stopwords, and varying Cases."
cleaned_tokens = preprocess_text_spacy(text)
print("Tokens:", cleaned_tokens)
